- identity 값인 input `x`가 단순히 layer를 통과하면서 `gradient vanishing`을 방지하거나 사라진 초기의 기억을 계속 전파할 수 있게 하는 역할로 생각했어요. 논문을 다시 읽어보니 F(x)의 아웃풋을 H(x)가 아닌 `H(x)-x`로 제한을하면서 각 Residual block이 자신만의 작은 목표에 집중하도록 만드는 것 같네요! x를 미분하면 1이니까 연산도 쉬워지니까 일석이조인 것 같습니다!
